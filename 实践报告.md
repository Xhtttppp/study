# YoLo目标检测算法学习报告
## 一、理论学习
### 1.什么是目标检测？
```
  目标检测的任务是同时识别图像中的物体并确定它们的位置，，模型需要找出物体在图像中的位置，通常用矩形框表示，并识别物体属于哪一类别。
    对于YoLo算法，输入的是一张图片，输出的是边界框坐标（归一化后的位置坐标及框的长和宽）、每个物体所属的类别，以及模型对物体检测结果的信心程度。
```
### 2.yoLo的核心思想
``` 
  YOLO的核心思想是将目标检测问题视为一个回归问题，即通过回归来预测目标的位置和类别。YOLO算法将目标检测的任务转化为回归问题，通过神经网络直接回归目标的位置、类别及置信度。
  与传统的目标检测方法（例如Faster R-CNN）不同，YOLO是一个一阶段模型，所有目标检测任务在一次前向传播中完成，其不需要像传统方法那样先生成区域提议（region proposals）再进行分类和回归，而是直接预测所有的边界框。输入图像经过一个深度卷积神经网络（Backbone 和 Neck 部分），一次性提取整个图像的特征图。
  YoLo将图像划分为S×S的网格，每个网格会预测多个边界框，YOLO在每个网格中预测的边界框数量通常是固定的。这些边界框的预测包括物体的坐标、尺寸以及该边界框内是否存在物体的置信度。在最初的yolov1中，每个网格只输出一种类别的概率，后续的模型引入Anchor Box，可以检测多个类别不同的物体。
  YOLO通过将目标检测问题转化为回归问题，并在一阶段完成预测，极大地提高了推理速度。这使得YOLO非常适合应用于实时检测场景，例如视频监控、无人驾驶等领域。
  YOLO通过将目标检测问题转化为回归问题，并在一阶段完成预测，极大地提高了推理速度，这使得YOLO非常适合应用于实时检测场景，例如视频监控、无人驾驶等领域。
  ```
  ### 3.网络架构
  ```
    YOLO的网络结构可以分为两个主要部分：Backbone和Detection Head。Backbone负责提取图像中的特征，YOLO通常使用如Darknet等卷积神经网络作为骨干网络，进行图像特征提取。Detection Head负责根据提取的特征进行目标检测，生成最终的边界框、置信度以及类别预测。
    原始YOLOv1输出一个S×S×(B×5+C)的张量，其中S×S是图像划分的网格大小；B 是每个网格预测的边界框数；5是每个边界框的预测值：4个坐标（x、y、宽度w、高度h）和1个置信度分数；C是目标类别数（例如C=80对于COCO数据集）。后续的模型对输出的张量进行了修改，引入了锚框和多尺度预测。
```
## 二、环境配置
![alt text](环境配置成功.png)

## 三、实践任务
### 1.使用YOLOv8预训练模型检测图像中的物体
相关代码和检测结果放于task1文件夹中，其中使用到的数据集为COCO8。
### 2.对视频或摄像头实时流进行目标检测
下载一个公开的视频作为检测对象，逐帧对视频作检测，相关代码和视频放于task2文件夹中。
### 3.在自己的数据集上训练YOLO模型
从Roboflow数据集平台上获取yolo格式的公开数据集用来训练YoLO模型，数据路径为：
https://app.roboflow.com/ds/HJaDQsOJ80?key=pleZVZUeIP

该数据集共约两千张图片，对5类物体进行检测，分别是['boots', 'gloves', 'helmet', 'human', 'vest']，
相关代码和运行结果放于task4文件夹中。
### 4.数据标注工具使用
## 四、进阶任务
### 1.YoLo12初步
使用yolo12n模型在前述数据集上训练，相关代码和运行结果放于task4文件夹中。
### 2.小目标检测优化
在不修改Ultralytics源码的情况下，放大图像尺寸，将yolo11n模型的原始分类损失函数更改为Focal Loss，使模型更关注难分类的样本。

在GitHub上获取公开的无人机目标检测数据集VisDrone-DET dataset，数据路径为：https://pan.baidu.com/s/1K-JtLnlHw98UuBDrYJvw3A

https://pan.baidu.com/s/1jdK_dAxRJeF2Xi50IoML1g

对原始数据中的txt文本作处理，使之适配yolo模型，并自行补充data.yaml文件。相关代码和运行结果放于文件task5中。
## 五、结果分析
### 1.YoLov8n预训练模型检测图像和视频
  检测结果显示，模型在图像中绘制了边界框，标注了识别到的类别以及置信度，但是与已知的txt文本对比，发现仍有个别物体未检测到，可能由于物体过小，边界框过密等原因导致置信度偏低，表现为未检测到。

在处理后的视频中，模型检测到物体，标以类别、置信度、边界框，并实时更新，效果较好，完成了检测功能。
### 2.利用开源数据集训练YoLov8n模型
通过混淆矩阵和各类损失函数可看出手套的精度、召回率均处于较低水平，其他物体识别效果较好，随着训练的进行，各类损失函数都呈下降趋势，精度、召回率和mAP都在上升，训练结束时mAP在50%的阈值下达到了0.855，但仍有提升空间。

整体上，模型对多数类别的检测效果良好，尤其是“helmet”、“human”和“vest”类别，精度和召回率都很高，而在“gloves”类别上表现不会，影响模型整体性能，这可能是手套这类物品本身尺寸小，不易识别，数据集中的gloves数量少，模型没有很好地学习到gloves的特征等因素导致的。
### 3.利用开源数据集训练YoLo12模型
使用YoLo12模型的训练结果与YoLov8n类似，但是各项损失均有稍许上升，mAP50有稍许下降，与YoLov8n，相比，在gloves类上的偏差更大，造成该区别的原因可能与YoLo12区域注意力机制、R-ELAN的引入等网络架构上的不同有关，导致模型在某些特定物体上的检测效果较差。
### 4.小目标检测优化
在对通用的yolo11n模型进行改进后，用VisDrone数据集来从零开始训练模型，可以看到，由于模型未加载任何权重，最初训练时，mAP50接近为0，各类损失很大，训练初期，mAP迅速上升，损失快速下降，随着训练的进行，模型趋于稳定，最终的mAP50约为0.387，根据混淆矩阵可看到在汽车和行人这两类上识别的准确率相对较高，而在三轮车、自行车等类别上检测效果较差。这与图像复杂度、图像中的样本数量偏差大等原因有关，模型还有很大的改进空间。
## 六、学习心得
### 1.了解了yolo的原理、基本架构和运行机制
### 2.明白了衡量模型性能的各类指标的含义
### 3.知道了yolo格式数据集的特定格式和各类参数含义，以及如何获取/转换yolo类型的数据集
### 4.模型内各参数的含义，以及如何合理设置参数
### 5.区分.pt和.yaml后缀的模型，前者是已预训练的模型，后者只有模型框架，没有权重，同时从零训练模型时需设置pretrained=False
### 6.学会运用一些开源工具寻找并标记数据
### 7.了解不同yolo模型之间的区别，理解每次迭代的改进之处，如C2f模块的不断优化
### 8.尝试使用labelImg绘制矩形框时直接卡退，在采取降版本等方式后成功使用
### 9.用yolo11n作小目标检测时，除了前述提到的方法，还可以考虑进一步扩大imgsz，使用CIoU Loss作为边界框损失函数，减小下采样次数（可尝试将Backbone中部分卷积层的stride由2调整为1，引入空洞卷积，增加P2检测头）等方法改善模型。实践时尝试使用CIoU Loss作为边界框损失函数，但受到$\text{Ultralytics}$ 框架的高度封装和内部动态加载机制，内部组件路径和命名不确定性，以及$\text{Callback Hook}$ 的时序问题，既不能简单导入损失函数库来解决，也不能通过回调和补丁来解决，最终使用默认的损失函数，在小目标检测上效果欠佳。